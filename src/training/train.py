import torch as T

from torch.utils.data._utils.collate import default_collate
from src.training.divergence import MMD_loss

import numpy as np
from scipy.stats import ks_2samp

def ks_test_tensors(tensor1, tensor2):
    """
    Performs the Kolmogorov-Smirnov test on two PyTorch tensors.

    Args:
        tensor1: The first PyTorch tensor.
        tensor2: The second PyTorch tensor.

    Returns:
        A tuple containing the KS statistic and the p-value.
    """
    # Convert tensors to NumPy arrays for scipy.stats.ks_2samp
    statistic, pvalue = ks_2samp(tensor1.numpy(), tensor2.numpy())
    return statistic, pvalue

from scipy.spatial.distance import cdist

def energy_distance(X, Y):
    XX = cdist(X, X).mean()
    YY = cdist(Y, Y).mean()
    XY = cdist(X, Y).mean()
    return 2 * XY - XX - YY

from sklearn.linear_model import LogisticRegression

def classifier_js_divergence(X, Y):
    X_all = np.vstack([X, Y])
    y_all = np.array([0]*len(X) + [1]*len(Y))
    
    clf = LogisticRegression(max_iter=1000).fit(X_all, y_all)
    probs = clf.predict_proba(X_all)[:, 1]
    
    eps = 1e-12
    probs = np.clip(probs, eps, 1 - eps)
    
    js = np.mean(np.log(2 * probs[:len(X)])) + np.mean(np.log(2 * (1 - probs[len(X):])))
    return -js


def train_ncm(model, dataloader, optimizer, device, num_epochs=10):
    model.to(device)
    model.train()
    ordered_v = model.v

    for epoch in range(1, num_epochs+1):
        epoch_loss = 0.0
        for batch in dataloader:
            # if DataLoader gives you back a list of samples, collate it
            if isinstance(batch, list):
                batch = default_collate(batch)

            # now batch is a dict of batched tensors
            # Batch = real data (so the data generated by M*)
            batch = {k: v.to(device) for k, v in batch.items()}
            batch_size = next(iter(batch.values())).shape[0]
            # Data generated by \hat{M}
            ncm_batch = model(batch_size)

            # convert them into matrices
            data_matrix = T.cat([batch[k] for k in ordered_v], axis=1)
            ncm_matrix = T.cat([ncm_batch[k] for k in ordered_v], axis=1)

            optimizer.zero_grad()
            # MMD loss compares the probability distributions of the two matrices
            loss = MMD_loss(data_matrix.float(),ncm_matrix,gamma=1)
            loss.backward()
            optimizer.step()
            

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(dataloader)
        print(f"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}")

    return model

def compute_accuracy(model, dataloader, device, target_var):
    model.eval()
    total_loss = 0
    total_e = 0
    total_js = 0
    with T.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            batch_size = next(iter(batch.values())).shape[0]

            preds=model(n=batch_size,select={target_var})[target_var]
            labels = batch[target_var]
            
            labels = T.cat([batch[target_var]], axis=1)
            pred_labels = T.cat([preds], axis=1)
            total_loss += MMD_loss(labels,pred_labels)

            total_e += energy_distance(labels,pred_labels)
            total_js += classifier_js_divergence(labels,pred_labels)
    print(f'\t\t energy-based: {1-(total_e/len(dataloader)):.4f}')
    print(f'\t\t js-divergence: {1-(total_js/len(dataloader)):.4f}')
    return 1-(total_loss/len(dataloader))

def print_accuracy(var, trained_ncm, train_dataloader, test_dataloader):
    train_acc = compute_accuracy(trained_ncm, train_dataloader, 'cpu', var)
    print(f'Final train accuracy for {var}: {train_acc:.4f}')

    test_acc = compute_accuracy(trained_ncm, test_dataloader, 'cpu', var)
    print(f'Final test accuracy  for {var}: {test_acc:.4f}')

